---
title: "Analyzing Pilot Results"
output:
  html_document:
    number_sections: false
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

```{r load-data-define-variables}

rm(list=ls())

pacman::p_load(pacman,
               rio,
               tidyverse,
               rstatix,
               DT,
               kableExtra,
               readr,
               writexl,
               jsonlite,
               stringr,
               gridExtra,
               knitr,
               magrittr,
               pdist,
               gghighlight)


# Load the data ##########################
session_results_all_ptp <- import(
        './results/pilots/preprocessed_data/session_results_long_form.csv'
        )

session_results_all_ptp <- session_results_all_ptp %>%
        reorder_levels(condition, order = c('practice',
                                            'practice2',
                                            'schema_c',
                                            'schema_ic',
                                            'landmark_schema',
                                            'random_locations',
                                            'no_schema')
                       )


```

```{r define-parameters-load-functions}

# Load an external script which contains functions for estimating either just the learning rate, or also the asymptote
source('./scripts/utils/functions_for_fitting_learning_curves.R')

# Create parameters as starting points for estimations
a_start <- 0.5
c_start <- 0.1

# Create lower and upper bound constraints on the asymptote and learning rate
a_lower <- 0
a_upper <- 1
c_lower <- 0
c_upper <- 20

```

```{r exclude-qc-fail-participants}

# Exclude the one participant that did not pay attention to instructions
session_results_all_ptp <-
        session_results_all_ptp %>%
        filter(ptp != '609478fa9e5b4d075246cfaf') %>%
        droplevels()

```


```{r create-long-form-accuracy-type}

session_results_all_ptp_long_accuracy <- 
        session_results_all_ptp %>%
        pivot_longer(cols = starts_with("correct_"),
                     names_to = 'accuracy_type',
                     values_to = 'accuracy_value') %>%
        mutate(accuracy_type = as.factor(accuracy_type)) %>%
        reorder_levels(accuracy_type, order = c(
                'correct_exact',
                'correct_one_square_away',
                'correct_rad_21',
                'correct_rad_42',
                'correct_rad_63',
                'correct_rad_84',
                'correct_rad_105'
        ))
 
```

# Do all the calculations

Create a giant, long form data with dependent variables being performance on each of the 8 repetition of PAs. 
Factors will include:
- condition
- accuracy_type: i.e. how wide is the accuracy box/radius
- neighbor_status: is the dependent variable reflecting performance on the neighbor PAs, island PAs, or collapsed over them?

```{r order-of-conditions}

# For each participant, list the order of conditions
condition_orders <- tibble(.rows = 7)

all_ptp <- unique(session_results_all_ptp$ptp)

for (iPtp in as.vector(all_ptp)){
        iPtp
        condition_orders[iPtp] <-
                unique(
                        session_results_all_ptp$condition[
                                session_results_all_ptp$ptp==iPtp
                                ])
}

```

```{r create-one-large-long-form-for-image-repetitions}

mean_by_rep_long <- 
        session_results_all_ptp_long_accuracy %>%
        filter(!condition %in% c('practice','practice2')) %>%
        droplevels() %>%
        group_by(ptp_trunk,
                 condition,
                 new_pa_img_row_number_across_sessions,
                 accuracy_type) %>%
        summarize(correct_mean = mean(accuracy_value, na.rm = T),
                  correct_sd = sd(accuracy_value, na.rm = T)) %>%
        ungroup()

# Calculate mean for neighbor vs non neighbor
mean_by_landmark_rep_long <-
        session_results_all_ptp_long_accuracy %>%
        filter(!condition %in% c('practice','practice2')) %>%
        droplevels() %>%
        group_by(ptp_trunk,
                 condition,
                 adjascent_neighbor,
                 new_pa_img_row_number_across_sessions,
                 accuracy_type) %>%
        summarize(correct_mean = mean(accuracy_value, na.rm = T),
                  correct_sd = sd(accuracy_value, na.rm = T)) %>%
        ungroup() %>%
        mutate(correct_mean = 
                       case_when(
                               is.na(adjascent_neighbor) ~ as.numeric(NA),
                               TRUE ~ correct_mean
                               ),
               correct_sd =
                       case_when(
                               is.na(adjascent_neighbor) ~ as.numeric(NA),
                               TRUE ~ correct_sd
                       ),
               )

# Pivot into wide form, so we can later merge with the other data reflecting overall performance
mean_by_landmark_rep_long_wide <- mean_by_landmark_rep_long %>%
        filter(!condition %in% c('random_locations',
                                 'no_schema')) %>%
        droplevels() %>%
        pivot_wider(id_cols = c(ptp_trunk,
                                condition,
                                new_pa_img_row_number_across_sessions,
                                accuracy_type),
                    values_from = c(correct_mean,
                                    correct_sd),
                    names_from = adjascent_neighbor,
                    names_prefix = 'neighbor_'
        )

# Now merge into one giant dataset
mean_by_rep_all_types <- merge(mean_by_rep_long,
                               mean_by_landmark_rep_long_wide,
                               by = c('ptp_trunk',
                                      'condition',
                                      'new_pa_img_row_number_across_sessions',
                                      'accuracy_type'),
                               all = TRUE)

# Pivot longer, but we have to do two columns, so break this up into two parts, then merge.
mean_by_rep_all_types_long_1 <-
        mean_by_rep_all_types %>%
        select(-contains('sd')) %>% 
        rename(both = correct_mean,
               island = correct_mean_neighbor_FALSE,
               neighbor = correct_mean_neighbor_TRUE) %>%
        pivot_longer(cols = c('both','island','neighbor'),
                     names_to = 'neighbor_status',
                     values_to = 'correct_mean',
                )
mean_by_rep_all_types_long_2 <-
        mean_by_rep_all_types %>%
        select(-contains('mean')) %>%
        rename(both = correct_sd,
               island = correct_sd_neighbor_FALSE,
               neighbor = correct_sd_neighbor_TRUE) %>%        
        pivot_longer(cols = c('both','island','neighbor'),
                     names_to = 'neighbor_status',
                     values_to = 'correct_sd',
                )
mean_by_rep_all_types_long <-
        merge(mean_by_rep_all_types_long_1,
              mean_by_rep_all_types_long_2,
              by = c('ptp_trunk',
                     'condition',
                     'new_pa_img_row_number_across_sessions',
                     'accuracy_type',
                     'neighbor_status'))

```

```{r all-learning-fits}




```


```{r plot, fig.width=10, fig.height=10, warning=FALSE, message=FALSE}

for (iPart in unique(session_results_all_ptp_long_accuracy$ptp_trunk)[1]){
        print(iPart)
        
        fig_long_accu <- mean_by_rep_all_types_long %>%
                filter(ptp_trunk == iPart) %>%
                droplevels() %>%
                ggplot(aes(x=new_pa_img_row_number_across_sessions,
                           y=correct_mean,
                           color=accuracy_type)) +
                
                # Add the average across toys
                geom_point(aes(shape = neighbor_status)) +
                geom_line(size=1,
                          aes(linetype = neighbor_status)) +
                
                # # Add the average across landmark or not
                # geom_point(data = mean_by_landmark_rep, 
                #            aes(group=adjascent_neighbor,
                #                color=adjascent_neighbor,
                #                y=correct_rad_63_mean)) +
                # geom_line(data = mean_by_landmark_rep, 
                #           aes(group=adjascent_neighbor,
                #               color=adjascent_neighbor,
                #               y=correct_rad_63_mean),
                #           size=1) +
                # 
                facet_grid(accuracy_type~condition) +
                ggtitle(paste(iPart,'; ','Various Accuracy types',sep='')) +
                theme(legend.position = 'top') +
                xlab('Image repetition') +
                scale_x_continuous(breaks=seq(1:8)) + 
                geom_vline(xintercept = 4.5, linetype = 'dashed')
        
        print(fig_long_accu)

}



```

```{r each-participant-correct-exact-one-square-63-rad, fig.width=10, fig.height=10, warning=FALSE, message=FALSE}


fig_long_accu <- mean_by_rep_long %>%
        filter(accuracy_type %in% c('correct_exact',
                                    'correct_one_square_away',
                                    'correct_rad_42',
                                    'correct_rad_63')) %>%
        droplevels() %>%
        ggplot(aes(x=new_pa_img_row_number_across_sessions,
                   y=correct_mean,
                   color=accuracy_type)) +
        
        # Add the average across toys
        geom_point() +
        geom_line(size=1,
                  aes(linetype = accuracy_type)) +
        
        # # Add the average across landmark or not
        # geom_point(data = mean_by_landmark_rep, 
        #            aes(group=adjascent_neighbor,
        #                color=adjascent_neighbor,
        #                y=correct_rad_63_mean)) +
        # geom_line(data = mean_by_landmark_rep, 
        #           aes(group=adjascent_neighbor,
        #               color=adjascent_neighbor,
        #               y=correct_rad_63_mean),
        #           size=1) +
        # 
        facet_grid(ptp_trunk~condition) +
        ggtitle(paste('Various Accuracy types',sep='')) +
        theme(legend.position = 'top') +
        xlab('Image repetition') +
        scale_x_continuous(breaks=seq(1:8)) + 
        geom_vline(xintercept = 4.5, linetype = 'dashed')

print(fig_long_accu)


```

# Averaged across participants


```{r plot-averaged-over-participants, fig.width=15, fig.height=5, warning=FALSE, message=FALSE}

# Calculate mean across participants for all data
mean_by_rep_across_ptp <-
        session_results_all_ptp %>%
        filter(!condition %in% c('practice','practice2')) %>%
        droplevels() %>%
        group_by(condition,new_pa_img_row_number_across_sessions) %>%
        summarize(correct_rad_63_mean = mean(correct_rad_63, na.rm = T),
                  correct_rad_63_sd   = sd(correct_rad_63, na.rm = T),
                  n = n(),
                  correct_rad_63_95_CI = 1.96*sd(correct_rad_63)/sqrt(n())) %>%
        ungroup()

# Now for landmarks and non-landmakr items
mean_by_landmark_rep_across_ptp <-
        session_results_all_ptp %>%
        filter(!condition %in% c('practice','practice2')) %>%
        droplevels() %>%
        group_by(condition,
                 adjascent_neighbor,
                 new_pa_img_row_number_across_sessions) %>%
        summarize(correct_rad_63_mean = mean(correct_rad_63, na.rm = T),
                  correct_rad_63_sd   = sd(correct_rad_63, na.rm = T),
                  n = n(),
                  correct_rad_63_95_CI = 1.96*sd(correct_rad_63)/sqrt(n())) %>%
        ungroup() %>%
        mutate(across(c(correct_rad_63_mean,
                        correct_rad_63_sd,
                        correct_rad_63_95_CI),
                      ~ case_when(
                              is.na(adjascent_neighbor) ~ as.numeric(NA),
                              TRUE ~ .
                      )))

fig_across_ptp <- mean_by_rep_across_ptp %>%
        ggplot(aes(x=new_pa_img_row_number_across_sessions,
                   y=correct_rad_63_mean)) +
        geom_point(alpha=0.2) +
        geom_line(size=2) +
        geom_ribbon(aes(ymin = correct_rad_63_mean-correct_rad_63_95_CI,
                        ymax = correct_rad_63_mean+correct_rad_63_95_CI),
                    alpha=0.2) +        
        
        # # Add the average across landmark or not
        geom_point(data = mean_by_landmark_rep_across_ptp,
                   aes(group=adjascent_neighbor,
                       color=adjascent_neighbor,
                       y=correct_rad_63_mean)) +
        geom_line(data = mean_by_landmark_rep_across_ptp,
                  aes(group=adjascent_neighbor,
                      color=adjascent_neighbor,
                      y=correct_rad_63_mean),
                  size=1) +

        facet_wrap(~condition, nrow = 1) +
        ggtitle(paste('Accuracy type: 63px radius',sep='')) +
        xlab('Image repetition') +
        ylab('63px Accuracy') +
        scale_x_continuous(breaks=seq(1:8)) +
        geom_vline(xintercept = 4.5, linetype = 'dashed') +
        theme(legend.position = 'top')

print(fig_across_ptp)

```

## Difference between landmark and non-landmark items:

Positive means landmark items were better than non-landmark ones.

```{r diff-between-neighbor-non-neighbor, fig.width=15, fig.height=4, warning=FALSE, message=FALSE}

# Calculate the difference between landmark and non-landmark items
mean_diff_neighbor_non_neighbor <- 
        mean_by_landmark_rep %>%
        filter(!condition %in% c('no_schema','random_locations')) %>%
        pivot_wider(id_cols = c(ptp_trunk,
                                condition,
                                new_pa_img_row_number_across_sessions),
                    names_from = adjascent_neighbor,
                    names_prefix = 'neighbor_',
                    values_from = correct_rad_63_mean) %>%
        mutate(neighbor_diff = neighbor_TRUE - neighbor_FALSE)
        
mean_diff_neighbor_non_neighbor_across_ptp <-
        mean_diff_neighbor_non_neighbor %>%
        group_by(condition,new_pa_img_row_number_across_sessions) %>%
        summarise(mean_diff_correct_rad_63 = mean(neighbor_diff),
                  n = n(),
                  ci_95 = 1.96*sd(neighbor_diff)/sqrt(n()))

# Plot the difference between landmark and non-landmark items
fig_neighbor_diff <- mean_diff_neighbor_non_neighbor_across_ptp %>%
        ggplot(aes(x=new_pa_img_row_number_across_sessions,
                   y=mean_diff_correct_rad_63)) +
        geom_line(size=1) +
        geom_ribbon(aes(ymin = mean_diff_correct_rad_63 - ci_95,
                        ymax = mean_diff_correct_rad_63 + ci_95),
                    alpha = 0.2) +
        
        
        facet_wrap(~condition, nrow = 1) +
        scale_x_continuous(breaks=seq(1:8)) +
        geom_hline(yintercept = 0, linetype = 'dashed') +
        geom_vline(xintercept = 4.5, linetype = 'dashed') +        
        ggtitle('Difference between landmark neighbor and non-neighbor items') +
        ylab('Neighbor VS non-neighbor') +
        xlab('Image repetition')

print(fig_neighbor_diff)

```

## Fit learning curves

I do two types of fitting:

- Red line: estimate only the learning rate
- Blue line: estimate the learning rate and asymptote.

In both cases, I constrain the search space:

- asymptote 0:1
- learning rate: 0 to 10

### Fit to all data

Seems like estimating both is better. So for the rest of the analysis, I will stick with estimating both.


```{r estimate-learning-curves, warning=FALSE, message=FALSE}

# Learning rate and asymptote across participants.
# This variable will contain the asymptote and learning rate for each condition, 
# since its using data averaged across participants
learning_and_asymptote_across_participants <- 
        mean_by_rep_across_ptp %>%
        group_by(condition) %>% 
        do(as.data.frame(
                optim(c(a_start,c_start),
                      fit_learning_and_asymptote,
                      gr = NULL,
                      seq(1,8),
                      .$correct_rad_63_mean,
                      method = 'L-BFGS-B',
                      lower = c(a_lower,c_lower),
                      upper = c(a_upper,c_upper)
                )) %>%
                   mutate(id = row_number()) %>%
                   pivot_wider(names_from = id,
                               values_from = par,
                               names_prefix = 'par_')) %>%
        rename(sse = value,
               n_iterations = counts,
               a = par_1,
               c = par_2) %>%
        ungroup()

# Now, generate predicted data based on the estimated learning rate and asymptote
learning_and_asymptote_across_participants_y_hat <- 
        learning_and_asymptote_across_participants %>%
        group_by(condition) %>% 
        mutate(y_hat_a_c = list(a*(1 - exp(-c*(seq(1:8)-1)))),
               new_pa_img_row_number_across_sessions = list(seq(1:8))) %>%
        unnest(c(y_hat_a_c,
                 new_pa_img_row_number_across_sessions)) %>%
        select(c(condition,
                 y_hat_a_c,
                 new_pa_img_row_number_across_sessions)) %>%
        ungroup()


# Merge with the real data, so we can plot efficiently
mean_by_rep_across_ptp <- merge(mean_by_rep_across_ptp,
                                learning_and_asymptote_across_participants_y_hat,
                                by = c('condition',
                                       'new_pa_img_row_number_across_sessions'))

# Now do the same but estimating only the learning rate:
learning_only_across_participants <- mean_by_rep_across_ptp %>%
        group_by(condition) %>%
        do(as.data.frame(
                optimize(fit_learning_only,
                      c(c_lower,c_upper),
                      seq(1,8),
                      .$correct_rad_63_mean
                      )) %>%
        rename(sse = objective,
               c = minimum)) %>%
        ungroup()

# Generate predicted data based on the learning rate estimate
learning_only_across_participants_y_hat <- 
        learning_only_across_participants %>%
        group_by(condition) %>% 
        mutate(y_hat_c = list(1 - exp(-c*(seq(1:8)-1))),
               new_pa_img_row_number_across_sessions = list(seq(1:8))) %>%
        unnest(c(y_hat_c,new_pa_img_row_number_across_sessions)) %>%
        select(c(condition,y_hat_c,new_pa_img_row_number_across_sessions)) %>%
        ungroup()


# Merge with the real data
mean_by_rep_across_ptp <- merge(mean_by_rep_across_ptp,
                                learning_only_across_participants_y_hat,
                                by = c('condition',
                                       'new_pa_img_row_number_across_sessions'))

```

```{r plot-learning-fits, fig.width=15, fig.height=5, warning=FALSE, message=FALSE}

# Plot the fits
fig_across_ptp <- mean_by_rep_across_ptp %>%
        ggplot(aes(x=new_pa_img_row_number_across_sessions,
                   y=correct_rad_63_mean)) +
        geom_point(alpha=0.2) +
        geom_line(size=2) +
        geom_ribbon(aes(ymin = correct_rad_63_mean-correct_rad_63_95_CI,
                        ymax = correct_rad_63_mean+correct_rad_63_95_CI),
                    alpha=0.2) +

        # Add the y_hat learning and asymptote
        geom_line(aes(x=new_pa_img_row_number_across_sessions,
                      y=y_hat_a_c),
                  size=1,
                  color='blue',
                  linetype = 'longdash') +
        
        # Add the y_hat learning only
        geom_line(aes(x=new_pa_img_row_number_across_sessions,
                      y=y_hat_c),
                  size=1,
                  color='red',
                  linetype = 'dotted') +

        facet_wrap(~condition, nrow = 1) +
        ggtitle(paste('Real and estimate data',sep='')) +
        xlab('Image repetition') +
        ylab('63px radius accuracy') +
        scale_x_continuous(breaks=c(1,2,3,4,5,6,7,8)) +
        # theme(legend.position = 'top') +
        geom_vline(xintercept = 4.5, linetype = 'dashed')


print(fig_across_ptp)

```

### Fit to landmark/non-landmark separately

Dotted line is learning-rate fit.
Dashed line is learning-rate + asymptote fit.


```{r estimate-learning-curves-landmark-non-landmark}

# Learning rate and asymptote for landmark neighbor trials
learning_and_asymptote_across_participants_landmark <- 
        mean_by_landmark_rep_across_ptp %>%
        filter(!condition %in% c('random_locations','no_schema')) %>%
        group_by(condition,adjascent_neighbor) %>% 
        do(as.data.frame(
                optim(c(a_start,c_start),
                      fit_learning_and_asymptote,
                      gr = NULL,
                      seq(1,8),
                      .$correct_rad_63_mean,
                      method = 'L-BFGS-B',
                      lower = c(a_lower,c_lower),
                      upper = c(a_upper,c_upper)
                )) %>% 
                   mutate(id = row_number()) %>%
                   pivot_wider(names_from = id,
                               values_from = par,
                               names_prefix = 'par_')) %>% 
        rename(sse = value,
               n_iterations = counts,
               a = par_1,
               c = par_2) %>%
        ungroup()

# Now, generate predicted data based on the estimated learning rate and asymptote
learning_and_asymptote_across_participants_landmark_y_hat <- 
        learning_and_asymptote_across_participants_landmark %>%
        group_by(condition,adjascent_neighbor) %>% 
        mutate(y_hat_a_c = list(a*(1 - exp(-c*(seq(1:8)-1)))),
               new_pa_img_row_number_across_sessions = list(seq(1:8))) %>%
        unnest(c(y_hat_a_c,
                 new_pa_img_row_number_across_sessions)) %>%
        select(c(condition,
                 y_hat_a_c,
                 new_pa_img_row_number_across_sessions)) %>%
        ungroup()


# Merge with the real data, so we can plot efficiently
mean_by_landmark_rep_across_ptp <- merge(mean_by_landmark_rep_across_ptp,
                                learning_and_asymptote_across_participants_landmark_y_hat,
                                by = c('condition','adjascent_neighbor',
                                       'new_pa_img_row_number_across_sessions'),
                                all = TRUE)

# Now do the same but estimating only the learning rate: ######################
learning_only_across_participants_landmark <- mean_by_landmark_rep_across_ptp %>%
        filter(!condition %in% c('random_locations','no_schema')) %>%
        group_by(condition,adjascent_neighbor) %>%
        do(as.data.frame(
                optimize(fit_learning_only,
                      c(c_lower,c_upper),
                      seq(1,8),
                      .$correct_rad_63_mean
                      )) %>%
        rename(sse = objective,
               c = minimum)) %>%
        ungroup()

# Generate predicted data based on the learning rate estimate
learning_only_across_participants_landmark_y_hat <- 
        learning_only_across_participants_landmark %>%
        group_by(condition,adjascent_neighbor) %>% 
        mutate(y_hat_c = list(1 - exp(-c*(seq(1:8)-1))),
               new_pa_img_row_number_across_sessions = list(seq(1:8))) %>%
        unnest(c(y_hat_c,new_pa_img_row_number_across_sessions)) %>%
        select(c(condition,y_hat_c,new_pa_img_row_number_across_sessions)) %>%
        ungroup()


# Merge with the real data
mean_by_landmark_rep_across_ptp <- merge(mean_by_landmark_rep_across_ptp,
                                learning_only_across_participants_landmark_y_hat,
                                by = c('condition','adjascent_neighbor',
                                       'new_pa_img_row_number_across_sessions'),
                                all = TRUE)



```

```{r plot-learning-fits-landmark, fig.width=15, fig.height=5, warning=FALSE, message=FALSE}

# Plot the fits
fig_landmark_across_ptp <- mean_by_landmark_rep_across_ptp %>%
        filter(!condition %in% c('random_locations','no_schema')) %>%
        ggplot(aes(x=new_pa_img_row_number_across_sessions,
                   y=correct_rad_63_mean,
                   group = adjascent_neighbor,
                   color = adjascent_neighbor)) +
        geom_point(size=3) +
        geom_line(size=1.5) +

        # Add the y_hat learning and asymptote
        geom_line(aes(x=new_pa_img_row_number_across_sessions,
                      y=y_hat_a_c,
                      group = adjascent_neighbor,
                      color = adjascent_neighbor),
                  size=1.5,
                  linetype = 'longdash') +

        # Add the y_hat learning only
        geom_line(aes(x=new_pa_img_row_number_across_sessions,
                      y=y_hat_c,
                      group = adjascent_neighbor,
                      color = adjascent_neighbor),
                  size=1.5,
                  linetype = 'dotted') +

        facet_wrap(~condition, nrow = 1) +
        ggtitle(paste('Real and estimate data',sep='')) +
        xlab('Image repetition') +
        ylab('63px radius accuracy') +
        scale_x_continuous(breaks=c(1,2,3,4,5,6,7,8)) +
        # theme(legend.position = 'top') +
        geom_vline(xintercept = 4.5, linetype = 'dashed')


print(fig_landmark_across_ptp)



```



```{r learning-curves-for-each-participant}

# Learning rate and asymptote for landmark neighbor trials
learning_and_asymptote_each_participant <- 
        mean_by_rep %>%
        group_by(ptp_trunk,condition) %>%
        do(as.data.frame(
                optim(c(a_start,c_start),
                      fit_learning_and_asymptote,
                      gr = NULL,
                      seq(1,8),
                      .$correct_rad_63_mean,
                      method = 'L-BFGS-B',
                      lower = c(a_lower,c_lower),
                      upper = c(a_upper,c_upper)
                )) %>% 
                   mutate(id = row_number()) %>%
                   pivot_wider(names_from = id,
                               values_from = par,
                               names_prefix = 'par_')) %>% 
        rename(sse = value,
               n_iterations = counts,
               a = par_1,
               c = par_2) %>%
        ungroup()

# Now, generate predicted data based on the estimated learning rate and asymptote
learning_and_asymptote_each_participant_y_hat <- 
        learning_and_asymptote_each_participant %>%
        group_by(ptp_trunk,condition) %>% 
        mutate(y_hat_a_c = list(a*(1 - exp(-c*(seq(1:8)-1)))),
               new_pa_img_row_number_across_sessions = list(seq(1:8))) %>%
        unnest(c(y_hat_a_c,
                 new_pa_img_row_number_across_sessions)) %>%
        select(c(condition,
                 y_hat_a_c,
                 new_pa_img_row_number_across_sessions)) %>%
        ungroup()


# Merge with the real data, so we can plot efficiently
mean_by_rep <- merge(mean_by_rep,
                     learning_and_asymptote_each_participant_y_hat,
                     by = c('ptp_trunk',
                            'condition',
                            'new_pa_img_row_number_across_sessions'))

# Now do the same but estimating only the learning rate: ######################
learning_only_each_participant <- 
        mean_by_rep %>%
        group_by(ptp_trunk,condition) %>%
        do(as.data.frame(
                optimize(fit_learning_only,
                      c(c_lower,c_upper),
                      seq(1,8),
                      .$correct_rad_63_mean
                      )) %>%
        rename(sse = objective,
               c = minimum)) %>%
        ungroup()

# Generate predicted data based on the learning rate estimate
learning_only_each_participant_y_hat <- 
        learning_only_each_participant %>%
        group_by(ptp_trunk,condition) %>% 
        mutate(y_hat_c = list(1 - exp(-c*(seq(1:8)-1))),
               new_pa_img_row_number_across_sessions = list(seq(1:8))) %>%
        unnest(c(y_hat_c,new_pa_img_row_number_across_sessions)) %>%
        select(c(condition,y_hat_c,new_pa_img_row_number_across_sessions)) %>%
        ungroup()


# Merge with the real data
mean_by_rep <- merge(mean_by_rep,
                     learning_only_each_participant_y_hat,
                     by = c('ptp_trunk',
                            'condition',
                            'new_pa_img_row_number_across_sessions'))

```


```{r plot-learning-fits-each-participant, fig.width=15, fig.height=15, warning=FALSE, message=FALSE}

# Plot the fits
fig_each_ptp <- mean_by_rep %>%
        ggplot(aes(x=new_pa_img_row_number_across_sessions,
                   y=correct_rad_63_mean)) +
        geom_point() +
        geom_line(size=1) +

        # Add the y_hat learning and asymptote
        geom_line(aes(x=new_pa_img_row_number_across_sessions,
                      y=y_hat_a_c),
                  size=1,
                  color='blue',
                  linetype = 'longdash') +
        
        # Add the y_hat learning only
        geom_line(aes(x=new_pa_img_row_number_across_sessions,
                      y=y_hat_c),
                  size=1,
                  color='red',
                  linetype = 'dotted') +

        facet_grid(ptp_trunk~condition) +
        ggtitle(paste('Real and estimate data',sep='')) +
        xlab('Image repetition') +
        ylab('63px radius accuracy') +
        scale_x_continuous(breaks=c(1,2,3,4,5,6,7,8)) +
        # theme(legend.position = 'top') +
        geom_vline(xintercept = 4.5, linetype = 'dashed') +
        geom_text(data=learning_and_asymptote_each_participant,
                  aes(x=6.5,y=0.25, label = paste('a=',as.character(round(a,2)),' ','c=',as.character(round(c,2)),seq=''))) + 
        geom_text(data=learning_only_each_participant,
                  aes(x=7,y=0.05, label = paste('c=',as.character(round(c,2)),seq='')))        
        # annotate('text',x=1.5,y = 0.85, label = learning_and_asymptote_each_participant$a ,size=3)


print(fig_each_ptp)



```

```{r estimate-learning-curves-landmark-non-landmark-each-participant}

# Learning rate and asymptote for landmark neighbor trials
learning_and_asymptote_each_participant_landmark <- 
        mean_by_landmark_rep %>%
        filter(!condition %in% c('random_locations','no_schema')) %>%
        group_by(ptp_trunk,condition,adjascent_neighbor) %>% 
        do(as.data.frame(
                optim(c(a_start,c_start),
                      fit_learning_and_asymptote,
                      gr = NULL,
                      seq(1,8),
                      .$correct_rad_63_mean,
                      method = 'L-BFGS-B',
                      lower = c(a_lower,c_lower),
                      upper = c(a_upper,c_upper)
                )) %>% 
                   mutate(id = row_number()) %>%
                   pivot_wider(names_from = id,
                               values_from = par,
                               names_prefix = 'par_')) %>% 
        rename(sse = value,
               n_iterations = counts,
               a = par_1,
               c = par_2) %>%
        ungroup()

# Now, generate predicted data based on the estimated learning rate and asymptote
learning_and_asymptote_each_participant_landmark_y_hat <- 
        learning_and_asymptote_each_participant_landmark %>%
        group_by(ptp_trunk,condition,adjascent_neighbor) %>% 
        mutate(y_hat_a_c = list(a*(1 - exp(-c*(seq(1:8)-1)))),
               new_pa_img_row_number_across_sessions = list(seq(1:8))) %>%
        unnest(c(y_hat_a_c,
                 new_pa_img_row_number_across_sessions)) %>%
        select(c(condition,
                 y_hat_a_c,
                 new_pa_img_row_number_across_sessions)) %>%
        ungroup()


# Merge with the real data, so we can plot efficiently
mean_by_landmark_rep <- merge(mean_by_landmark_rep,
                              learning_and_asymptote_each_participant_landmark_y_hat,
                              by = c('ptp_trunk',
                                     'condition',
                                     'adjascent_neighbor',
                                     'new_pa_img_row_number_across_sessions'),
                              all = TRUE)

# Now do the same but estimating only the learning rate: ######################
learning_only_each_participant_landmark <- 
        mean_by_landmark_rep %>%
        filter(!condition %in% c('random_locations','no_schema')) %>%
        group_by(ptp_trunk,condition,adjascent_neighbor) %>%
        do(as.data.frame(
                optimize(fit_learning_only,
                      c(c_lower,c_upper),
                      seq(1,8),
                      .$correct_rad_63_mean
                      )) %>%
        rename(sse = objective,
               c = minimum)) %>%
        ungroup()

# Generate predicted data based on the learning rate estimate
learning_only_each_participant_landmark_y_hat <- 
        learning_only_each_participant_landmark %>%
        group_by(ptp_trunk,condition,adjascent_neighbor) %>% 
        mutate(y_hat_c = list(1 - exp(-c*(seq(1:8)-1))),
               new_pa_img_row_number_across_sessions = list(seq(1:8))) %>%
        unnest(c(y_hat_c,new_pa_img_row_number_across_sessions)) %>%
        select(c(condition,y_hat_c,new_pa_img_row_number_across_sessions)) %>%
        ungroup()


# Merge with the real data
mean_by_landmark_rep <- merge(mean_by_landmark_rep,
                                learning_only_each_participant_landmark_y_hat,
                                by = c('ptp_trunk',
                                       'condition',
                                       'adjascent_neighbor',
                                       'new_pa_img_row_number_across_sessions'),
                                all = TRUE)



```

```{r plot-learning-fits-landmark-each-participant, fig.width=15, fig.height=25, warning=FALSE, message=FALSE}

# Plot the fits
fig_landmark_each_ptp <- mean_by_landmark_rep %>%
        filter(!condition %in% c('random_locations','no_schema')) %>%
        ggplot(aes(x=new_pa_img_row_number_across_sessions,
                   y=correct_rad_63_mean,
                   group = adjascent_neighbor,
                   color = adjascent_neighbor)) +
        geom_point(size=3,alpha=0.3) +
        geom_line(size=1.5,alpha=0.3) +

        # Add the y_hat learning and asymptote
        geom_line(aes(x=new_pa_img_row_number_across_sessions,
                      y=y_hat_a_c,
                      group = adjascent_neighbor,
                      color = adjascent_neighbor),
                  size=1.5,
                  linetype = 'longdash') +

        # Add the y_hat learning only
        geom_line(aes(x=new_pa_img_row_number_across_sessions,
                      y=y_hat_c,
                      group = adjascent_neighbor,
                      color = adjascent_neighbor),
                  size=1.5,
                  linetype = 'dotted') +

        facet_grid(ptp_trunk~condition) +
        ggtitle(paste('Real and estimate data',sep='')) +
        xlab('Image repetition') +
        ylab('63px radius accuracy') +
        scale_x_continuous(breaks=c(1,2,3,4,5,6,7,8)) +
        # theme(legend.position = 'top') +
        geom_vline(xintercept = 4.5, linetype = 'dashed')


print(fig_landmark_each_ptp)



```

```{r scatterplot-estimated-parameters, fig.height=5, fig.width=8}

learning_and_asymptote_each_participant %>%
        filter(ptp_trunk != '609478f...') %>%
        ggplot(aes(x=condition,y=a)) +
        geom_violin(width=0.3) +
        geom_dotplot(binaxis='y', stackdir='center',
                     stackratio=1, dotsize=0.5, fill="black") +        
        # geom_point(aes(color = ptp_trunk)) +
        geom_line(aes(group=ptp_trunk,
                      color=ptp_trunk),
                  alpha=0.5) +
        stat_summary(fun=mean, 
                     geom="point", 
                     shape=20, 
                     size=5, 
                     color="red", 
                     fill="red") + 
        stat_summary(fun=median, 
                     geom="crossbar", 
                     width=0.2,
                     color="black") +          
        ylab('Asymptote estimate') +
        ggtitle('Estimating both rate and asymptote')

learning_and_asymptote_each_participant %>%
        filter(ptp_trunk != '609478f...') %>%
        ggplot(aes(x=condition,y=c)) +
        geom_violin() +
        geom_dotplot(binaxis='y', stackdir='center',
                     stackratio=1, dotsize=0.5, fill="black") +        
        # geom_point(aes(color = ptp_trunk)) +
        geom_line(aes(group=ptp_trunk,
                      color=ptp_trunk),
                  alpha=0.5) +
        stat_summary(fun=mean, 
                     geom="point", 
                     shape=20, 
                     size=5, 
                     color="red", 
                     fill="red") + 
        stat_summary(fun=median, 
                     geom="crossbar", 
                     width=0.2,
                     color="black") +          
        ylab('Learning rate estimate') +
        ggtitle('Estimating both rate and asymptote')

learning_only_each_participant %>%
        filter(ptp_trunk != '609478f...') %>%
        ggplot(aes(x=condition,y=c)) +
        geom_violin() +
        geom_dotplot(binaxis='y', stackdir='center',
                     stackratio=1, dotsize=0.5, fill="black") +        
        # geom_point(aes(color = ptp_trunk)) +
        geom_line(aes(group=ptp_trunk,
                      color=ptp_trunk),
                  alpha=0.5) +
        stat_summary(fun=mean, 
                     geom="point", 
                     shape=20, 
                     size=5, 
                     color="red", 
                     fill="red") + 
        stat_summary(fun=median, 
                     geom="crossbar", 
                     width=0.2,
                     color="black") +          
        ylab('Learning rate estimate') +
        ggtitle('Estimating only the learning rate')




```

